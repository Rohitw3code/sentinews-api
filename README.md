# ğŸš€ SentiNews Server

<div align="center">

![SentiNews](https://img.shields.io/badge/SentiNews-Financial%20News%20Intelligence-blue?style=for-the-badge&logo=trending-up)

**AI-Powered Financial News Sentiment Analysis Backend**

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=flat-square&logo=python&logoColor=white)](https://python.org)
[![Flask](https://img.shields.io/badge/Flask-2.0+-000000?style=flat-square&logo=flask&logoColor=white)](https://flask.palletsprojects.com)
[![SQLite](https://img.shields.io/badge/SQLite-3-003B57?style=flat-square&logo=sqlite&logoColor=white)](https://sqlite.org)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-412991?style=flat-square&logo=openai&logoColor=white)](https://openai.com)
[![LangChain](https://img.shields.io/badge/LangChain-Framework-1C3C3C?style=flat-square&logo=chainlink&logoColor=white)](https://langchain.com)

</div>

---

## ğŸ“‹ Table of Contents

- [ğŸŒŸ Overview](#-overview)
- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸ“Š API Endpoints](#-api-endpoints)
- [ğŸ¤– AI Models](#-ai-models)
- [ğŸ•·ï¸ Web Scrapers](#ï¸-web-scrapers)
- [ğŸ“ˆ Pipeline Management](#-pipeline-management)
- [ğŸ”’ Security](#-security)
- [ğŸ“š Usage Examples](#-usage-examples)
- [ğŸ› ï¸ Development](#ï¸-development)
- [ğŸ› Troubleshooting](#-troubleshooting)
- [ğŸ“„ License](#-license)

---

## ğŸŒŸ Overview

SentiNews Server is a sophisticated backend system that powers financial news sentiment analysis using cutting-edge AI technology. It automatically scrapes financial news from multiple sources, analyzes sentiment using advanced language models, and provides comprehensive APIs for accessing insights.

### ğŸ¯ Key Capabilities

- **ğŸ” Intelligent News Scraping**: Automated collection from multiple financial news sources
- **ğŸ§  Dual Sentiment Analysis**: Separate financial and overall sentiment scoring
- **ğŸ¢ Entity Recognition**: Automatic detection of companies and cryptocurrencies
- **ğŸ“Š Real-time Analytics**: Live sentiment tracking and trend analysis
- **ğŸ”„ Automated Pipelines**: Scheduled data collection and analysis
- **ğŸŒ RESTful API**: Comprehensive endpoints for data access

---

## âœ¨ Features

### ğŸ¤– AI-Powered Analysis
- **Multi-Model Support**: OpenAI GPT-4 and Groq Llama integration
- **Structured Output**: Pydantic-validated sentiment analysis
- **Cost Tracking**: Detailed usage and cost monitoring
- **Retry Logic**: Robust error handling and recovery

### ğŸ•·ï¸ Advanced Web Scraping
- **Dynamic Scraper Management**: Modular scraper architecture
- **Multiple Sources**: Zawya, MENAbytes, and extensible framework
- **Content Extraction**: Clean text extraction and preprocessing
- **Duplicate Prevention**: Intelligent deduplication

### ğŸ“Š Data Management
- **SQLite Database**: Efficient local storage
- **Structured Schema**: Optimized for analytics queries
- **Data Integrity**: Foreign key constraints and validation
- **Migration Support**: Schema evolution capabilities

### ğŸ”„ Pipeline Automation
- **Scheduled Execution**: Configurable daily runs
- **Real-time Status**: Live pipeline monitoring
- **Graceful Shutdown**: Safe interruption handling
- **Progress Tracking**: Detailed execution metrics

---

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    A[Web Scrapers] --> B[Raw Articles]
    B --> C[Content Processing]
    C --> D[AI Sentiment Analysis]
    D --> E[SQLite Database]
    E --> F[REST API]
    F --> G[Frontend Dashboard]
    
    H[Scheduler] --> I[Pipeline Manager]
    I --> A
    
    J[Configuration] --> K[API Keys]
    K --> D
```

### ğŸ§© Core Components

| Component | Description | Technology |
|-----------|-------------|------------|
| **API Server** | RESTful endpoints and request handling | Flask + CORS |
| **Sentiment Analyzer** | AI-powered sentiment analysis engine | LangChain + OpenAI/Groq |
| **Scraper Manager** | Dynamic web scraper orchestration | Requests + BeautifulSoup |
| **Database Layer** | Data persistence and retrieval | SQLite3 |
| **Pipeline Engine** | Automated data processing workflows | APScheduler |
| **Configuration** | Environment and runtime settings | Python-dotenv |

---

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

```bash
# Python 3.8 or higher
python --version

# pip package manager
pip --version
```

### âš¡ Installation

1. **Clone and Navigate**
   ```bash
   git clone <repository-url>
   cd server
   ```

2. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Environment Setup**
   ```bash
   # Create .env file
   cp .env.example .env
   
   # Edit with your API keys
   nano .env
   ```

4. **Initialize Database**
   ```bash
   python -c "import database; database.create_database()"
   ```

5. **Start Server**
   ```bash
   python api.py
   ```

### ğŸŒ Verify Installation

```bash
# Test API health
curl http://localhost:5000/

# Check available scrapers
curl http://localhost:5000/api/scrapers

# View pipeline status
curl http://localhost:5000/api/pipeline_status
```

---

## ğŸ“ Project Structure

```
server/
â”œâ”€â”€ ğŸ“„ api.py                    # Main Flask application
â”œâ”€â”€ ğŸ“„ database.py               # Database operations
â”œâ”€â”€ ğŸ“„ pipeline.py               # Data processing pipelines
â”œâ”€â”€ ğŸ“„ main.py                   # CLI entry point
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“„ .env.example              # Environment template
â”œâ”€â”€ ğŸ“„ README.md                 # This file
â”‚
â”œâ”€â”€ ğŸ“ analysis/                 # AI analysis modules
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â””â”€â”€ ğŸ“„ sentiment_analyzer.py # Core sentiment analysis
â”‚
â””â”€â”€ ğŸ“ scrapers/                 # Web scraping modules
    â”œâ”€â”€ ğŸ“„ __init__.py
    â”œâ”€â”€ ğŸ“„ scraper_manager.py    # Dynamic scraper management
    â”œâ”€â”€ ğŸ“„ zawya_scraper.py      # Zawya.com scraper
    â””â”€â”€ ğŸ“„ menabytes_scraper.py  # MENAbytes.com scraper
```

---

## ğŸ”§ Configuration

### ğŸ”‘ Environment Variables

Create a `.env` file in the server directory:

```env
# AI Model Configuration
OPENAI_API_KEY=your_openai_api_key_here
GROQ_API_KEY=your_groq_api_key_here

# Pipeline Security
PIPELINE_PASSWORD=your_secure_pipeline_password

# Optional: Model Preferences
DEFAULT_LLM_PROVIDER=openai
DEFAULT_OPENAI_MODEL=gpt-4o-mini
DEFAULT_GROQ_MODEL=llama3-8b-8192
```

### âš™ï¸ Runtime Configuration

| Setting | Default | Description |
|---------|---------|-------------|
| `schedule_time` | `01:00` | Daily pipeline execution time (UTC) |
| `max_retries` | `3` | AI analysis retry attempts |
| `cache_duration` | `300` | Response cache time (seconds) |
| `rate_limit` | `100/hour` | API request rate limiting |

---

## ğŸ“Š API Endpoints

### ğŸ  Core Endpoints

#### `GET /` - API Information
```json
{
  "message": "Welcome to the Financial News Sentiment API",
  "endpoints": { ... }
}
```

#### `GET /api/scrapers` - Available Scrapers
```json
["zawya.com", "menabytes.com"]
```

### ğŸ”„ Pipeline Management

#### `POST /api/trigger_pipeline` - Start Pipeline
```bash
curl -X POST http://localhost:5000/api/trigger_pipeline \
  -H "Content-Type: application/json" \
  -d '{
    "password": "your_password",
    "provider": "openai",
    "model_name": "gpt-4o-mini",
    "scrapers": ["zawya.com", "menabytes.com"]
  }'
```

#### `GET /api/pipeline_status` - Pipeline Status
```json
{
  "is_running": true,
  "status": "Analyzing sentiment",
  "progress": 15,
  "total": 50,
  "current_task": "Analyzing article ID: 123"
}
```

#### `POST /api/stop_pipeline` - Stop Pipeline
```bash
curl -X POST http://localhost:5000/api/stop_pipeline \
  -H "Content-Type: application/json" \
  -d '{"password": "your_password"}'
```

### ğŸ“° Data Access

#### `GET /api/articles` - Fetch Articles
```bash
# All articles
curl "http://localhost:5000/api/articles"

# Filtered articles
curl "http://localhost:5000/api/articles?entity_name=Apple&entity_type=company&limit=10"
```

#### `GET /api/entities` - List Entities
```json
[
  {"entity_name": "Apple", "entity_type": "company"},
  {"entity_name": "Bitcoin", "entity_type": "crypto"}
]
```

#### `GET /api/top_entities` - Top Entities by Sentiment
```bash
curl "http://localhost:5000/api/top_entities?sentiment_type=overall&sentiment=positive&limit=10"
```

### ğŸ“ˆ Analytics

#### `GET /api/sentiment_over_time` - Sentiment Trends
```bash
curl "http://localhost:5000/api/sentiment_over_time?entity_name=Tesla"
```

#### `GET /api/dashboard_stats` - Dashboard Statistics
```json
{
  "total_entities": 150,
  "articles_analyzed": 1250,
  "total_sentiment_points": 2100,
  "sentiment_distribution": {
    "positive": 800,
    "negative": 600,
    "neutral": 700
  }
}
```

#### `GET /api/summarize_entity` - AI Entity Summary
```bash
curl "http://localhost:5000/api/summarize_entity?entity_name=Apple"
```

### ğŸ“Š Usage Analytics

#### `GET /api/usage_stats` - API Usage Statistics
```bash
# Detailed logs
curl "http://localhost:5000/api/usage_stats"

# Summary by provider
curl "http://localhost:5000/api/usage_stats?summarize=true"
```

---

## ğŸ¤– AI Models

### ğŸ§  Supported Providers

#### OpenAI Integration
- **Models**: GPT-4, GPT-4-turbo, GPT-4o-mini
- **Features**: Structured output, cost tracking, rate limiting
- **Use Case**: High-accuracy sentiment analysis

#### Groq Integration  
- **Models**: Llama3-8b-8192, Mixtral-8x7b
- **Features**: Fast inference, cost-effective
- **Use Case**: High-volume processing

### ğŸ“ Sentiment Analysis Schema

```python
class EntitySentiment(BaseModel):
    entity_name: str          # Full company/crypto name
    entity_type: str          # "company" or "crypto"
    financial_sentiment: str  # "positive", "negative", "neutral"
    overall_sentiment: str    # "positive", "negative", "neutral"
    reasoning: str           # AI explanation
```

### ğŸ’° Cost Optimization

- **Token Usage Tracking**: Real-time monitoring
- **Model Selection**: Automatic cost-performance optimization
- **Batch Processing**: Efficient API usage
- **Retry Logic**: Minimize failed requests

---

## ğŸ•·ï¸ Web Scrapers

### ğŸ—ï¸ Scraper Architecture

Each scraper module implements:
- `SOURCE_NAME`: Unique identifier
- `get_article_urls()`: URL discovery
- `scrape_article_content(url)`: Content extraction

### ğŸ“° Supported Sources

#### Zawya.com Scraper
```python
# Features
- Business news focus
- Arabic/English content
- Author attribution
- Publication dates
- Clean text extraction
```

#### MENAbytes.com Scraper
```python
# Features  
- MENA tech/business news
- Startup coverage
- Investment news
- Clean content parsing
```

### ğŸ”§ Adding New Scrapers

1. **Create Scraper Module**
   ```python
   # scrapers/newsource_scraper.py
   SOURCE_NAME = "newsource.com"
   
   def get_article_urls():
       # Implementation
       return urls
   
   def scrape_article_content(url):
       # Implementation
       return article_data
   ```

2. **Auto-Discovery**
   - Place in `scrapers/` directory
   - Follow naming convention: `*_scraper.py`
   - Implement required functions

3. **Test Integration**
   ```bash
   curl http://localhost:5000/api/scrapers
   ```

---

## ğŸ“ˆ Pipeline Management

### ğŸ”„ Pipeline Workflow

```mermaid
sequenceDiagram
    participant S as Scheduler
    participant P as Pipeline
    participant SC as Scrapers
    participant AI as AI Analyzer
    participant DB as Database
    
    S->>P: Trigger Daily Run
    P->>SC: Fetch Article URLs
    SC->>P: Return New URLs
    P->>SC: Scrape Content
    SC->>P: Return Articles
    P->>AI: Analyze Sentiment
    AI->>P: Return Analysis
    P->>DB: Store Results
    P->>S: Complete
```

### â° Scheduling

```python
# Configure schedule
POST /api/configure_schedule
{
  "password": "your_password",
  "schedule_time": "02:30"  # UTC time
}
```

### ğŸ“Š Monitoring

```python
# Real-time status
GET /api/pipeline_status

# Historical runs
GET /api/pipeline_last_run
```

### ğŸ›‘ Control Operations

```python
# Manual trigger
POST /api/trigger_pipeline

# Graceful stop
POST /api/stop_pipeline

# Status check
GET /api/pipeline_status
```

---

## ğŸ”’ Security

### ğŸ” Authentication

- **Pipeline Operations**: Password-protected endpoints
- **API Keys**: Secure environment variable storage
- **Rate Limiting**: Request throttling (planned)

### ğŸ›¡ï¸ Data Protection

- **Input Validation**: Pydantic schema validation
- **SQL Injection**: Parameterized queries
- **CORS**: Configurable cross-origin policies
- **Error Handling**: Secure error responses

### ğŸ”‘ Best Practices

```bash
# Strong passwords
PIPELINE_PASSWORD=your_very_secure_password_here

# API key rotation
OPENAI_API_KEY=sk-...  # Rotate regularly

# Environment isolation
# Never commit .env files
echo ".env" >> .gitignore
```

---

## ğŸ“š Usage Examples

### ğŸš€ Basic Pipeline Run

```python
import requests

# Trigger analysis
response = requests.post('http://localhost:5000/api/trigger_pipeline', 
    json={
        'password': 'your_password',
        'provider': 'openai',
        'scrapers': ['zawya.com']
    }
)

# Monitor progress
while True:
    status = requests.get('http://localhost:5000/api/pipeline_status').json()
    print(f"Status: {status['status']} ({status['progress']}/{status['total']})")
    if not status['is_running']:
        break
    time.sleep(5)
```

### ğŸ“Š Data Analysis

```python
# Get sentiment trends
trends = requests.get('http://localhost:5000/api/sentiment_over_time', 
    params={'entity_name': 'Apple'}).json()

# Analyze top performers
top_positive = requests.get('http://localhost:5000/api/top_entities',
    params={
        'sentiment_type': 'financial',
        'sentiment': 'positive',
        'limit': 10
    }).json()

# Generate AI summary
summary = requests.get('http://localhost:5000/api/summarize_entity',
    params={'entity_name': 'Tesla'}).json()
```

### ğŸ” Custom Queries

```python
# Filter articles by multiple criteria
articles = requests.get('http://localhost:5000/api/articles', 
    params={
        'entity_type': 'crypto',
        'financial_sentiment': 'positive',
        'limit': 20
    }).json()

# Get entity-specific articles
entity_articles = requests.get('http://localhost:5000/api/entity_articles_by_sentiment',
    params={
        'entity_name': 'Bitcoin',
        'entity_type': 'crypto'
    }).json()
```

---

## ğŸ› ï¸ Development

### ğŸ§ª Testing

```bash
# Run basic tests
python -m pytest tests/

# Test specific scraper
python -c "from scrapers import zawya_scraper; print(zawya_scraper.get_article_urls())"

# Test sentiment analysis
python -c "from analysis.sentiment_analyzer import SentimentAnalyzer; analyzer = SentimentAnalyzer(); print(analyzer.analyze_text_for_sentiment('Apple stock rises on strong earnings.'))"
```

### ğŸ”§ Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Enable debug mode
export FLASK_ENV=development
export FLASK_DEBUG=1

# Run with auto-reload
python api.py
```

### ğŸ“ Code Style

```bash
# Format code
black .

# Lint code  
flake8 .

# Type checking
mypy .
```

### ğŸ› Debugging

```python
# Enable verbose logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Database inspection
sqlite3 news_data.db
.tables
.schema articles
SELECT COUNT(*) FROM articles;
```

---

## ğŸ› Troubleshooting

### âŒ Common Issues

#### Database Errors
```bash
# Reset database
rm news_data.db
python -c "import database; database.create_database()"
```

#### API Key Issues
```bash
# Verify API keys
python -c "import os; from dotenv import load_dotenv; load_dotenv(); print('OpenAI:', bool(os.getenv('OPENAI_API_KEY'))); print('Groq:', bool(os.getenv('GROQ_API_KEY')))"
```

#### Scraper Failures
```bash
# Test individual scrapers
python -c "from scrapers.zawya_scraper import get_article_urls; print(len(get_article_urls()))"
```

#### Memory Issues
```bash
# Monitor memory usage
pip install psutil
python -c "import psutil; print(f'Memory: {psutil.virtual_memory().percent}%')"
```

### ğŸ” Debug Mode

```python
# Enable detailed logging
import logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

### ğŸ“ Support

- **Issues**: Create GitHub issue with logs
- **Features**: Submit feature request
- **Documentation**: Contribute improvements

---

## ğŸ“ˆ Performance Metrics

### ğŸš€ Benchmarks

| Metric | Value | Notes |
|--------|-------|-------|
| **Articles/Hour** | ~500 | With parallel processing |
| **Analysis Speed** | ~2s/article | GPT-4o-mini average |
| **Memory Usage** | ~200MB | Typical operation |
| **Database Size** | ~50MB | Per 10K articles |

### ğŸ“Š Optimization Tips

- **Batch Processing**: Group API calls
- **Caching**: Enable response caching
- **Model Selection**: Use appropriate AI model
- **Parallel Scrapers**: Run multiple scrapers

---

## ğŸ”® Roadmap

### ğŸ¯ Upcoming Features

- [ ] **Real-time Streaming**: WebSocket support
- [ ] **Advanced Analytics**: ML trend prediction  
- [ ] **Multi-language**: Support for Arabic content
- [ ] **API Authentication**: JWT token system
- [ ] **Horizontal Scaling**: Redis clustering
- [ ] **Advanced Scrapers**: Social media integration

### ğŸš€ Performance Improvements

- [ ] **Async Processing**: FastAPI migration
- [ ] **Database Optimization**: PostgreSQL support
- [ ] **Caching Layer**: Redis integration
- [ ] **Load Balancing**: Multi-instance support

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built with â¤ï¸ for Financial Intelligence**

[![GitHub](https://img.shields.io/badge/GitHub-Repository-181717?style=flat-square&logo=github)](https://github.com/your-repo)
[![Documentation](https://img.shields.io/badge/Docs-Latest-blue?style=flat-square&logo=gitbook)](https://docs.your-domain.com)
[![API](https://img.shields.io/badge/API-v1.0-green?style=flat-square&logo=swagger)](http://localhost:5000/)

</div>